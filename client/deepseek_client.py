#!/usr/bin/env python3
"""
DeepSeek APIå®¢æˆ·ç«¯
"""

import os
import json
import re
import time
from typing import List, Dict, Optional
import requests

import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

import config

# DeepSeek APIé…ç½®
DEEPSEEK_API_BASE = "https://api.deepseek.com/chat/completions"
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

# Tokenå’Œä¸Šä¸‹æ–‡é…ç½®
# DeepSeek APIæ”¯æŒæœ€å¤§128K tokensä¸Šä¸‹æ–‡ï¼Œä½†ä¸ºäº†ç¨³å®šæ€§å’Œæˆæœ¬æ§åˆ¶ï¼Œå»ºè®®è®¾ç½®å¦‚ä¸‹ï¼š
# æ³¨æ„ï¼šä¸åŒè½®æ¬¡çš„æ•°æ®é‡å·®å¼‚å¾ˆå¤§
# - 1è½®å¯¹è¯ï¼šæ¯æ¡æ•°æ®çº¦400-600 tokensï¼Œ50æ¡çº¦20000-30000 tokens
# - 5è½®å¯¹è¯ï¼šæ¯æ¡æ•°æ®çº¦800-1200 tokensï¼Œ50æ¡çº¦40000-60000 tokens
# DeepSeek APIçš„max_tokensé™åˆ¶ï¼š
# - æ ‡å‡†æ¨¡å‹ï¼š8000
# - reasoneræ¨¡å‹ï¼šé»˜è®¤32Kï¼Œæœ€å¤§64K

# åŸºç¡€é…ç½®
MAX_TOKENS_OUTPUT_REASONER_MAX = 64000  # reasoneræ¨¡å‹æœ€å¤§è¾“å‡ºtokenæ•°
MAX_TOKENS_OUTPUT_STANDARD = 8000  # æ ‡å‡†æ¨¡å‹çš„æœ€å¤§è¾“å‡ºtokenæ•°
MAX_CONTEXT_LENGTH = 110000  # ä¼šè¯æ€»é•¿åº¦é™åˆ¶ï¼ˆç•™å‡ºçº¦18K bufferï¼Œé¿å…æ„å¤–è¶…é™ï¼‰

# Reasoneræ¨¡å‹æ¨ç†tokené…ç½®
REASONING_TOKENS_BUFFER = 5000  # reasoneræ¨¡å‹æ¨ç†è¿‡ç¨‹éœ€è¦çš„é¢å¤–tokenæ•°ï¼ˆå›ºå®šå¸¸æ•°ï¼‰
# è¯´æ˜ï¼šåŸºäºå®é™…åœºæ™¯åˆ†æï¼Œæ¨ç†tokené€šå¸¸å å†…å®¹tokençš„20-30%
# å¯¹äº50æ¡æ•°æ®ï¼ˆ5è½®ï¼‰ï¼Œæ¨ç†tokençº¦éœ€8000+ï¼Œä½†è€ƒè™‘åˆ°ï¼š
# 1. å¤§å¤šæ•°ä»»åŠ¡è§„æ¨¡è¾ƒå°ï¼ˆ1-10æ¡ï¼‰ï¼Œ5000è¶³å¤Ÿ
# 2. å¤§è§„æ¨¡ä»»åŠ¡å¯é€šè¿‡ä»£ç çš„è‡ªåŠ¨é‡è¯•æœºåˆ¶å¤„ç†
# 3. 5000 tokensçº¦ä¸º50æ¡5è½®æ•°æ®å†…å®¹tokençš„10-15%ï¼Œæä¾›åˆç†çš„å®‰å…¨è¾¹é™…

# æ¨¡å‹é€‰æ‹©ï¼šdeepseek-chatï¼ˆæ ‡å‡†ï¼‰æˆ– deepseek-reasonerï¼ˆæ¨ç†æ¨¡å¼ï¼Œè´¨é‡æ›´é«˜ä½†ç¨æ…¢ï¼‰
# é»˜è®¤ä½¿ç”¨ deepseek-reasoner ä»¥ä¿è¯ç”Ÿæˆè´¨é‡
MODEL_NAME = os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner")  # å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–

# ç”Ÿæˆå‚æ•°ï¼ˆä¸ç½‘é¡µç‰ˆä¿æŒä¸€è‡´ï¼‰
TEMPERATURE = 0.7  # æ§åˆ¶éšæœºæ€§ï¼Œ0.7æ˜¯ç½‘é¡µç‰ˆå¸¸ç”¨å€¼


class DeepSeekClient:
    """DeepSeek APIå®¢æˆ·ç«¯
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä½¿ç”¨system messageå­˜å‚¨å›ºå®šçš„initial promptï¼Œåˆ©ç”¨Context Cachingæœºåˆ¶
    2. åªä¿ç•™å¿…è¦çš„å¯¹è¯å†å²ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
    3. æ™ºèƒ½ç®¡ç†ä¼šè¯é•¿åº¦ï¼Œåœ¨æ¥è¿‘é™åˆ¶æ—¶å¼€å¯æ–°ä¼šè¯
    """
    
    def __init__(self, api_key: str, model: str = None):
        self.api_key = api_key
        self.model = model or MODEL_NAME
        self.session_messages: List[Dict] = []
        self.system_prompt: Optional[str] = None  # å›ºå®šçš„system promptï¼Œç”¨äºç¼“å­˜
        self.current_tokens = 0
        self.initial_prompt_tokens = 0  # initial promptçš„tokenæ•°ï¼Œç”¨äºä¼°ç®—
        
    def _estimate_tokens(self, text: str) -> int:
        """ç²—ç•¥ä¼°ç®—tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼‰"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)
    
    def _check_if_need_new_session(self, message: str, estimated_output_tokens: int) -> bool:
        """æ£€æŸ¥å‘é€æŒ‡å®šæ¶ˆæ¯æ˜¯å¦éœ€è¦å¼€å¯æ–°ä¼šè¯
        
        Args:
            message: è¦å‘é€çš„æ¶ˆæ¯
            estimated_output_tokens: é¢„ä¼°çš„è¾“å‡ºtokenæ•°
            
        Returns:
            Trueè¡¨ç¤ºéœ€è¦å¼€å¯æ–°ä¼šè¯ï¼ŒFalseè¡¨ç¤ºä¸éœ€è¦
        """
        # ä¼°ç®—æ¶ˆæ¯çš„tokenæ•°
        message_tokens = self._estimate_tokens(message)
        
        # è®¡ç®—å‘é€åé¢„æœŸçš„æ€»tokenæ•°
        expected_total = self.current_tokens + message_tokens + estimated_output_tokens
        
        # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œéœ€è¦å¼€å¯æ–°ä¼šè¯
        return expected_total >= MAX_CONTEXT_LENGTH
    
    def estimate_output_tokens(self, round_num: int, needed_count: int) -> int:
        """æ ¹æ®è½®æ¬¡å’Œéœ€è¦çš„æ•°æ®é‡ï¼Œä¼°ç®—è¾“å‡ºtokenæ•°ï¼ˆä¿å®ˆä¼°ç®—ï¼ŒåŠ bufferï¼‰
        
        ä½¿ç”¨åæ¯”ä¾‹å…³ç³»æ¥å¢åŠ bufferé‡ï¼š
        - æ•°æ®è¶Šå°‘ï¼Œbufferæ¯”ä¾‹è¶Šé«˜ï¼ˆæœ€é«˜50%ï¼‰
        - æ•°æ®è¶Šå¤šï¼Œbufferæ¯”ä¾‹è¶Šä½ï¼ˆæœ€ä½30%ï¼‰
        - 50æ¡æ—¶è¾¾åˆ°æœ€ä½æ¯”ä¾‹ï¼ˆ30%ï¼‰
        - è¶…å‡º50æ¡éƒ½æŒ‰30%è®¡ç®—
        
        å¯¹äºreasoneræ¨¡å‹ï¼Œé¢å¤–å¢åŠ å›ºå®šçš„æ¨ç†tokenå¸¸æ•°ã€‚
        
        Args:
            round_num: å¯¹è¯è½®æ¬¡ï¼ˆ1-5ï¼‰
            needed_count: éœ€è¦ç”Ÿæˆçš„æ•°æ®æ¡æ•°
            
        Returns:
            é¢„ä¼°çš„è¾“å‡ºtokenæ•°ï¼ˆä¿å®ˆä¼°ç®—ï¼Œå·²åŠ bufferï¼‰
        """
        # ä¼°ç®—æ¯æ¡æ•°æ®çš„tokenæ•°
        tokens_per_item = config.TOKENS_PER_ITEM_BY_ROUND.get(round_num, 1000)
        
        # è®¡ç®—bufferæ¯”ä¾‹ï¼ˆåæ¯”ä¾‹å…³ç³»ï¼‰
        # å½“needed_count = 1æ—¶ï¼Œbuffer = 50% (æœ€é«˜ï¼Œå³1.5å€)
        # å½“needed_count = 50æ—¶ï¼Œbuffer = 30% (æœ€ä½ï¼Œå³1.3å€)
        # å½“needed_count > 50æ—¶ï¼Œbuffer = 30%
        if needed_count >= 50:
            buffer_ratio = 1.3  # 30% buffer (1 + 0.3 = 1.3)
        else:
            buffer_ratio = 1.5 - 0.2 * (needed_count / 50)
        
        # è®¡ç®—åŸºç¡€tokenæ•°ï¼ˆå†…å®¹tokenï¼‰
        base_tokens = int(tokens_per_item * needed_count * buffer_ratio)
        
        # å¯¹äºreasoneræ¨¡å‹ï¼Œé¢å¤–å¢åŠ å›ºå®šçš„æ¨ç†tokenå¸¸æ•°
        if "reasoner" in self.model.lower():
            estimated_tokens = base_tokens + REASONING_TOKENS_BUFFER
        else:
            estimated_tokens = base_tokens
        
        return estimated_tokens
    
    def start_new_session(self, initial_prompt: str) -> None:
        """å¼€å¯æ–°ä¼šè¯ï¼Œç›´æ¥å°†initial_promptè®¾ç½®ä¸ºsystem message
        
        æ ¹æ®DeepSeek APIçš„å¤šè½®å¯¹è¯è§„èŒƒï¼Œç›´æ¥å°†system promptæ”¾å…¥messagesä¸­ã€‚
        åç»­çš„user messageå’Œassistant responseä¼šè¿½åŠ åˆ°messagesæ•°ç»„ä¸­ã€‚
        """
        self.session_messages = [{
            "role": "system",
            "content": initial_prompt
        }]
        self.system_prompt = initial_prompt
        self.initial_prompt_tokens = self._estimate_tokens(initial_prompt)
        self.current_tokens = self.initial_prompt_tokens
        print(f"âœ… æ–°ä¼šè¯å·²å¼€å¯ (system prompt: {self.initial_prompt_tokens} tokens)")
    
    def send_message(self, message: str, max_tokens: int) -> Optional[str]:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ï¼ˆå¤šè½®å¯¹è¯ï¼‰
        
        æ ¹æ®DeepSeek APIè§„èŒƒï¼Œå°†user messageæ·»åŠ åˆ°messagesæ•°ç»„ï¼Œå‘é€è¯·æ±‚åï¼Œ
        å°†assistant responseä¹Ÿæ·»åŠ åˆ°messagesæ•°ç»„ï¼Œå½¢æˆå®Œæ•´çš„å¯¹è¯å†å²ã€‚
        
        Args:
            message: è¦å‘é€çš„æ¶ˆæ¯
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆå¿…é¡»æŒ‡å®šï¼Œæ ¹æ®é¢„ä¼°è¾“å‡ºé•¿åº¦è®¾ç½®ï¼‰
            
        æ³¨æ„ï¼šè°ƒç”¨æ­¤æ–¹æ³•å‰åº”è¯¥å…ˆæ£€æŸ¥tokenæ˜¯å¦è¶³å¤Ÿï¼Œå¦‚æœä¸å¤Ÿåº”è¯¥å…ˆå¼€å¯æ–°ä¼šè¯
        """
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°messagesæ•°ç»„
        self.session_messages.append({
            "role": "user",
            "content": message
        })
        
        message_tokens = self._estimate_tokens(message)
        self.current_tokens += message_tokens
        
        # å‘é€è¯·æ±‚
        response = self._send_request(max_tokens=max_tokens)
        if response:
            # å°†assistantçš„å›å¤ä¹Ÿæ·»åŠ åˆ°messagesæ•°ç»„ï¼Œå½¢æˆå®Œæ•´çš„å¯¹è¯å†å²
            self.session_messages.append({
                "role": "assistant",
                "content": response
            })
            
            response_tokens = self._estimate_tokens(response)
            self.current_tokens += response_tokens
            return response
        return None
    
    def reset_session(self, initial_prompt: str) -> None:
        """é‡ç½®ä¼šè¯ï¼Œå‡†å¤‡æ–°çš„ä»»åŠ¡
        
        æ¯ä¸ªä»»åŠ¡å®Œæˆåï¼Œé‡ç½®ä¼šè¯ï¼Œåªä¿ç•™system promptï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚
        """
        self.start_new_session(initial_prompt)
    
    def ensure_session_ready(self, message: str, estimated_output_tokens: int) -> bool:
        """ç¡®ä¿ä¼šè¯æœ‰è¶³å¤Ÿçš„tokenå‘é€æ¶ˆæ¯ï¼Œå¦‚æœä¸å¤Ÿåˆ™å¼€å¯æ–°ä¼šè¯
        
        Args:
            message: è¦å‘é€çš„æ¶ˆæ¯
            estimated_output_tokens: é¢„ä¼°çš„è¾“å‡ºtokenæ•°
            
        Returns:
            Trueè¡¨ç¤ºå¼€å¯äº†æ–°ä¼šè¯ï¼ŒFalseè¡¨ç¤ºæ²¡æœ‰å¼€å¯ï¼ˆtokenè¶³å¤Ÿï¼‰
        """
        if self._check_if_need_new_session(message, estimated_output_tokens):
            print(f"âš ï¸  æ£€æµ‹åˆ°tokenä¸è¶³ ({self.current_tokens} tokens)ï¼Œå¼€å¯æ–°ä¼šè¯...")
            # ä½¿ç”¨ä¼ å…¥çš„initial_promptï¼ˆé¿å…é‡å¤è¯»å–æ–‡ä»¶ï¼‰
            # å¦‚æœself.system_promptå­˜åœ¨ï¼Œè¯´æ˜ä¹‹å‰å·²ç»è¯»å–è¿‡ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
            if self.system_prompt:
                initial_prompt = self.system_prompt
            else:
                initial_prompt = config.INITIAL_PROMPT_FILE.read_text(encoding='utf-8')
            self.start_new_session(initial_prompt)
            time.sleep(2)
            return True  # è¿”å›Trueè¡¨ç¤ºå¼€å¯äº†æ–°ä¼šè¯
        return False  # è¿”å›Falseè¡¨ç¤ºæ²¡æœ‰å¼€å¯æ–°ä¼šè¯ï¼ˆtokenè¶³å¤Ÿï¼‰
    
    def _send_request(self, max_tokens: int, use_json_mode: bool = True) -> Optional[str]:
        """å‘é€APIè¯·æ±‚
        
        Args:
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆå¿…é¡»æŒ‡å®šï¼Œæ ¹æ®é¢„ä¼°è¾“å‡ºé•¿åº¦è®¾ç½®ï¼‰
            use_json_mode: æ˜¯å¦ä½¿ç”¨JSONæ¨¡å¼ï¼ˆå¼ºåˆ¶è¾“å‡ºJSONæ ¼å¼ï¼‰ï¼Œé»˜è®¤True
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„max_tokensï¼ˆå·²ç»åœ¨è°ƒç”¨å‰æ ¹æ®é¢„ä¼°è¾“å‡ºé•¿åº¦è®¾ç½®å¥½ï¼‰
        max_output = max_tokens
        
        # ä¼°ç®—å½“å‰è¯·æ±‚çš„è¾“å…¥tokenæ•°ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯æ˜¾ç¤ºï¼‰
        estimated_input_tokens = self.current_tokens
        
        # æ„å»ºè¯·æ±‚æ•°æ®ï¼Œç¬¦åˆDeepSeek APIè§„èŒƒ
        data = {
            "model": self.model,
            "messages": self.session_messages,
            "temperature": TEMPERATURE,
            "max_tokens": max_output,
            "stream": False,  # ä¸ä½¿ç”¨æµå¼è¾“å‡º
        }
        
        # å¦‚æœä½¿ç”¨reasoneræ¨¡å‹ï¼Œå¯ç”¨æ€è€ƒæ¨¡å¼ä»¥ä¿è¯ç”Ÿæˆè´¨é‡
        # æ€è€ƒæ¨¡å¼ä¼šè®©æ¨¡å‹è¿›è¡Œæ¨ç†æ€è€ƒï¼Œç”Ÿæˆè´¨é‡æ›´é«˜ï¼Œä½†é€Ÿåº¦ç¨æ…¢
        if "reasoner" in self.model.lower():
            data["thinking"] = {
                "type": "enabled"  # å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œä¿è¯ç”Ÿæˆè´¨é‡
            }
        
        # ä½¿ç”¨JSONæ¨¡å¼å¯ä»¥ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
        # æ³¨æ„ï¼šä½¿ç”¨JSONæ¨¡å¼æ—¶ï¼Œpromptä¸­å¿…é¡»æ˜ç¡®è¦æ±‚ç”ŸæˆJSON
        if use_json_mode:
            data["response_format"] = {
                "type": "json_object"
            }
        
        try:
            # å‘é€è¯·æ±‚ï¼Œå¢åŠ è¿æ¥å’Œè¯»å–è¶…æ—¶è®¾ç½®
            # timeoutå‚æ•°è¯´æ˜ï¼š
            # - ç¬¬ä¸€ä¸ªå€¼(30): è¿æ¥è¶…æ—¶ï¼Œè¡¨ç¤ºå»ºç«‹TCPè¿æ¥çš„æœ€å¤§ç­‰å¾…æ—¶é—´
            #   å¦‚æœ30ç§’å†…æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œä¼šæŠ›å‡ºTimeoutå¼‚å¸¸
            # - ç¬¬äºŒä¸ªå€¼(1800): è¯»å–è¶…æ—¶ï¼Œè¡¨ç¤ºä»æœåŠ¡å™¨æ¥æ”¶æ•°æ®çš„æœ€å¤§ç­‰å¾…æ—¶é—´
            #   å¦‚æœ30åˆ†é’Ÿå†…æ²¡æœ‰æ”¶åˆ°ä»»ä½•æ•°æ®ï¼Œä¼šæŠ›å‡ºTimeoutå¼‚å¸¸
            #   å¯¹äºå¤§æ•°æ®é‡ç”Ÿæˆï¼Œéœ€è¦è¶³å¤Ÿé•¿çš„è¯»å–è¶…æ—¶æ—¶é—´
            response = requests.post(
                DEEPSEEK_API_BASE, 
                headers=headers, 
                json=data, 
                timeout=(30, 1800),  # (è¿æ¥è¶…æ—¶30ç§’, è¯»å–è¶…æ—¶1800ç§’=30åˆ†é’Ÿ)
                stream=False  # ä¸ä½¿ç”¨æµå¼ï¼Œç¡®ä¿å®Œæ•´æ¥æ”¶å“åº”
            )
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦å®Œæ•´
            if not response.content:
                print(f"âš ï¸  è­¦å‘Š: APIè¿”å›ç©ºå“åº”")
                return None
            
            # å°è¯•è§£æJSON
            try:
                result = response.json()
            except json.JSONDecodeError as json_err:
                print(f"âš ï¸  è­¦å‘Š: JSONè§£æå¤±è´¥ï¼Œå“åº”å¯èƒ½ä¸å®Œæ•´")
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                print(f"å“åº”å†…å®¹é•¿åº¦: {len(response.content)} bytes")
                print(f"è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
                
                # å°è¯•ä»å“åº”ä¸­æå–usageä¿¡æ¯ï¼ˆå³ä½¿JSONè§£æå¤±è´¥ï¼Œusageå¯èƒ½åœ¨å“åº”ä¸­ï¼‰
                try:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–usageä¿¡æ¯
                    usage_match = re.search(r'"usage"\s*:\s*\{[^}]+\}', response.text)
                    if usage_match:
                        usage_str = usage_match.group(0)
                        print(f"   æ£€æµ‹åˆ°usageä¿¡æ¯: {usage_str}")
                except:
                    pass
                
                print(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response.text[:500]}")
                print(f"JSONè§£æé”™è¯¯: {json_err}")
                return None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰Context Cachingå‘½ä¸­ï¼ˆç”¨äºç›‘æ§å’Œä¼˜åŒ–ï¼‰
            if "usage" in result:
                usage = result["usage"]
                prompt_tokens = usage.get("prompt_tokens", 0)
                completion_tokens = usage.get("completion_tokens", 0)
                total_tokens = usage.get("total_tokens", 0)
                cache_hit = usage.get("prompt_cache_hit_tokens", 0)
                cache_miss = usage.get("prompt_cache_miss_tokens", 0)
                
                # è·å–æ¨ç†tokenæ•°ï¼ˆreasoneræ¨¡å‹ï¼‰
                reasoning_tokens = 0
                if "completion_tokens_details" in usage:
                    completion_details = usage["completion_tokens_details"]
                    reasoning_tokens = completion_details.get("reasoning_tokens", 0)
                
                if cache_hit > 0:
                    total_prompt = cache_hit + cache_miss
                    hit_rate = cache_hit / total_prompt if total_prompt > 0 else 0
                    if hit_rate > 0.5:  # ç¼“å­˜å‘½ä¸­ç‡è¶…è¿‡50%æ—¶æ˜¾ç¤º
                        print(f"ğŸ’¾ Context Cacheå‘½ä¸­: {cache_hit}/{total_prompt} tokens ({hit_rate*100:.1f}%)")
                
                # æ˜¾ç¤ºè¯¦ç»†çš„tokenä½¿ç”¨æƒ…å†µ
                if total_tokens > 0:
                    token_info = f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={prompt_tokens}"
                    if cache_hit > 0:
                        token_info += f" (ç¼“å­˜å‘½ä¸­={cache_hit}, æœªå‘½ä¸­={cache_miss})"
                    token_info += f", è¾“å‡º={completion_tokens}"
                    if reasoning_tokens > 0:
                        token_info += f" (æ¨ç†={reasoning_tokens}, å†…å®¹={completion_tokens - reasoning_tokens})"
                    token_info += f", æ€»è®¡={total_tokens}, max_tokens={max_output}"
                    print(token_info)
            
            if "choices" in result and len(result["choices"]) > 0:
                choice = result["choices"][0]
                content = choice["message"]["content"]
                
                # æ£€æŸ¥finish_reasonå’Œtokenä½¿ç”¨æƒ…å†µ
                finish_reason = choice.get("finish_reason", "unknown")
                reasoning_tokens = 0
                content_tokens = 0
                if "usage" in result:
                    usage = result["usage"]
                    completion_tokens = usage.get("completion_tokens", 0)
                    if "completion_tokens_details" in usage:
                        completion_details = usage["completion_tokens_details"]
                        reasoning_tokens = completion_details.get("reasoning_tokens", 0)
                        content_tokens = completion_tokens - reasoning_tokens
                
                if finish_reason == "length":
                    # æ£€æŸ¥æ˜¯å¦æ˜¯reasoneræ¨¡å‹ä¸”åªæœ‰æ¨ç†tokenæ²¡æœ‰å†…å®¹token
                    if "reasoner" in self.model.lower() and reasoning_tokens > 0 and content_tokens == 0:
                        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºè¢«æˆªæ–­ï¼Œæ¨ç†tokenç”¨å®Œäº†æ‰€æœ‰max_tokensï¼ˆæ¨ç†={reasoning_tokens}ï¼‰ï¼Œæ²¡æœ‰å‰©ä½™tokenç”Ÿæˆå†…å®¹")
                        print(f"   å»ºè®®: éœ€è¦å¢åŠ max_tokensä»¥å®¹çº³æ¨ç†è¿‡ç¨‹å’Œå†…å®¹ç”Ÿæˆ")
                        # è¿”å›Noneï¼Œè®©è°ƒç”¨æ–¹çŸ¥é“éœ€è¦é‡è¯•
                        return None
                    else:
                        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºè¢«æˆªæ–­ï¼ˆè¾¾åˆ°max_tokensé™åˆ¶ï¼‰ï¼Œå¯èƒ½éœ€è¦å¢åŠ max_tokensæˆ–åˆ†æ‰¹ç”Ÿæˆ")
                elif finish_reason == "stop":
                    pass  # æ­£å¸¸å®Œæˆ
                else:
                    print(f"â„¹ï¸  å®ŒæˆåŸå› : {finish_reason}")
                
                # æ£€æŸ¥contentæ˜¯å¦ä¸ºç©ºï¼ˆreasoneræ¨¡å‹æ¨ç†ç”¨å°½tokenæ—¶å¯èƒ½è¿”å›Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼‰
                if content is None or (isinstance(content, str) and len(content.strip()) == 0):
                    print(f"âš ï¸  è­¦å‘Š: å“åº”å†…å®¹ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ¨ç†tokenç”¨å°½äº†æ‰€æœ‰max_tokens")
                    if reasoning_tokens > 0:
                        print(f"   æ¨ç†token: {reasoning_tokens}, å†…å®¹token: {content_tokens}")
                        print(f"   å½“å‰max_tokens={max_output}ï¼Œå»ºè®®è‡³å°‘å¢åŠ åˆ°{int(max_output * 1.5)}ä»¥å®¹çº³æ¨ç†å’Œå†…å®¹")
                    return None
                
                # å¦‚æœä½¿ç”¨JSONæ¨¡å¼ï¼Œcontentå­—æ®µç›´æ¥å°±æ˜¯JSONå­—ç¬¦ä¸²ï¼Œä¸éœ€è¦é¢å¤–æå–
                # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œä»ç„¶è¿”å›åŸå§‹contentï¼Œè®©extract_json_from_textå¤„ç†
                return content
            else:
                print(f"âŒ APIè¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                # å³ä½¿choicesä¸ºç©ºï¼Œä¹Ÿå°è¯•æ˜¾ç¤ºusageä¿¡æ¯
                if "usage" in result:
                    usage = result["usage"]
                    prompt_tokens = usage.get("prompt_tokens", 0)
                    completion_tokens = usage.get("completion_tokens", 0)
                    total_tokens = usage.get("total_tokens", 0)
                    print(f"   å“åº”tokenä¿¡æ¯: è¾“å…¥={prompt_tokens}, è¾“å‡º={completion_tokens}, æ€»è®¡={total_tokens}")
                return None
                
        except requests.exceptions.Timeout as e:
            print(f"âŒ APIè¯·æ±‚è¶…æ—¶: {e}")
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            print(f"   æç¤º: å¯èƒ½æ˜¯ç½‘ç»œè¿æ¥æ…¢æˆ–æœåŠ¡å™¨å“åº”æ—¶é—´é•¿ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œæˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
            return None
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ APIè¿æ¥é”™è¯¯: {e}")
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            print(f"   æç¤º: ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–æœåŠ¡å™¨ä¸å¯è¾¾")
            return None
        except requests.exceptions.HTTPError as e:
            print(f"âŒ APIè¯·æ±‚HTTPé”™è¯¯: {e}")
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.json()
                    print(f"   é”™è¯¯è¯¦æƒ…: {error_detail}")
                    # å°è¯•ä»é”™è¯¯å“åº”ä¸­æå–tokenä¿¡æ¯
                    if isinstance(error_detail, dict):
                        if "usage" in error_detail:
                            usage = error_detail["usage"]
                            prompt_tokens = usage.get("prompt_tokens", 0)
                            completion_tokens = usage.get("completion_tokens", 0)
                            total_tokens = usage.get("total_tokens", 0)
                            cache_hit = usage.get("prompt_cache_hit_tokens", 0)
                            cache_miss = usage.get("prompt_cache_miss_tokens", 0)
                            reasoning_tokens = 0
                            if "completion_tokens_details" in usage:
                                completion_details = usage["completion_tokens_details"]
                                reasoning_tokens = completion_details.get("reasoning_tokens", 0)
                            
                            token_info = f"   å“åº”tokenä¿¡æ¯: è¾“å…¥={prompt_tokens}"
                            if cache_hit > 0:
                                token_info += f" (ç¼“å­˜å‘½ä¸­={cache_hit}, æœªå‘½ä¸­={cache_miss})"
                            token_info += f", è¾“å‡º={completion_tokens}"
                            if reasoning_tokens > 0:
                                token_info += f" (æ¨ç†={reasoning_tokens}, å†…å®¹={completion_tokens - reasoning_tokens})"
                            token_info += f", æ€»è®¡={total_tokens}"
                            print(token_info)
                except:
                    print(f"   å“åº”çŠ¶æ€ç : {e.response.status_code}")
                    # å°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–usageä¿¡æ¯
                    try:
                        usage_match = re.search(r'"usage"\s*:\s*\{[^}]+\}', e.response.text)
                        if usage_match:
                            print(f"   æ£€æµ‹åˆ°usageä¿¡æ¯: {usage_match.group(0)}")
                    except:
                        pass
                    print(f"   å“åº”å†…å®¹: {e.response.text[:500]}")
            return None
        except requests.exceptions.ChunkedEncodingError as e:
            print(f"âŒ APIå“åº”æ¥æ”¶é”™è¯¯ï¼ˆå“åº”ä¸å®Œæ•´ï¼‰: {e}")
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            # å°è¯•ä»éƒ¨åˆ†å“åº”ä¸­æå–usageä¿¡æ¯
            try:
                if hasattr(e, 'response') and e.response is not None:
                    usage_match = re.search(r'"usage"\s*:\s*\{[^}]+\}', e.response.text)
                    if usage_match:
                        print(f"   æ£€æµ‹åˆ°usageä¿¡æ¯: {usage_match.group(0)}")
            except:
                pass
            print(f"   æç¤º: æœåŠ¡å™¨åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­å…³é—­äº†è¿æ¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–æœåŠ¡å™¨é—®é¢˜")
            print(f"   å»ºè®®: ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return None
        except requests.exceptions.RequestException as e:
            error_msg = str(e)
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            if "prematurely" in error_msg.lower() or "incomplete" in error_msg.lower():
                print(f"âŒ APIå“åº”ä¸å®Œæ•´: {e}")
                print(f"   æç¤º: å“åº”åœ¨å®Œå…¨æ¥æ”¶å‰ç»“æŸï¼Œå¯èƒ½æ˜¯ç½‘ç»œä¸­æ–­æˆ–æœåŠ¡å™¨æå‰å…³é—­è¿æ¥")
                print(f"   å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¨åé‡è¯•")
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ APIå“åº”JSONè§£æå¤±è´¥: {e}")
            print(f"   è¯·æ±‚tokenä¿¡æ¯: è¾“å…¥â‰ˆ{estimated_input_tokens}, max_tokens={max_output}")
            print(f"   æç¤º: å“åº”å¯èƒ½ä¸å®Œæ•´æˆ–æ ¼å¼é”™è¯¯")
            return None
